{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "弹幕(正面评论)： \n",
      " ['这个短片过激了啊，以目前为止你帮助别人会获得一声谢谢的\\r\\n', '我是真的没有遇到过这样的人\\r\\n', '有点以偏概全吧，我一次去广州，基本见过的所有人都会让座，道谢\\r\\n', '我遇到的都会说谢谢\\r\\n', '我遇到的人都会说谢谢，我想不是偶然吧\\r\\n', '纳尼?看来我运气还不错从来没遇到这种\\r\\n', '不会说谢谢的人我感觉没救了\\r\\n', '爱国 敬业 诚信 友善\\r\\n', '是奥我们确实要反思自己了\\r\\n', '现在的我做事不怎么主动，但是做好事的时候很主动']\n",
      "弹幕(负面评论)： \n",
      " ['本来就很冷漠\\r\\n', '人不如机器\\r\\n', '世间人心冷漠无情，只有这小天狗还有点温度\\r\\n', '我这周给小朋友让座被一个妇女给抢了。\\r\\n', '我好不容易才变成现在这样冷漠，又想骗我去学善良\\r\\n', '冷漠的社会\\r\\n', '快到而立之年的我，悟出一个道理，想在这个社会不受伤害，首先要学会恶毒\\r\\n', '我在公交给一对爷爷奶奶让座，站起来碍着另外一边爷爷下车了。结果边下车边对我冷嘲热讽\\r\\n', '我还是做机器人吧，太惨了\\r\\n', '人将跟远古单细胞生物一样慢慢变成其他生物的一部分而彻底丧失自我\\r\\n']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import jieba\n",
    "import codecs\n",
    "import re\n",
    "def load_data( pos_file, neg_file):\n",
    "    #加载数据\n",
    "    neg_docs = codecs.open(neg_file, 'r', 'utf-8').readlines()\n",
    "    pos_docs = codecs.open(pos_file, 'r', 'utf-8').readlines()\n",
    "    return pos_docs, neg_docs\n",
    "pos_tm,neg_tm= load_data( 'data/pos_file.txt', 'data/neg_file.txt')\n",
    "print(\"弹幕(正面评论)：\",'\\n',pos_tm)\n",
    "print(\"弹幕(负面评论)：\",'\\n',neg_tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wordCut(sentence, stop_words='data/stopwords.txt'):\n",
    "    # 将句子进行分词，包括去除停止词和标点符号\n",
    "    # 删除标点符号\n",
    "    remove_chars = '[·。，’!\"\\#$%&\\'()＃！（）*+,-./:;<=>?\\@，：?￥★、…．\\\n",
    "    ＞【】］《》？“”‘’\\[\\\\]^_`{|}~]+'\n",
    "    sentence = re.sub(remove_chars, \"\", sentence)\n",
    "    stop_words = [word.strip() for word in open(stop_words, 'r')]\n",
    "    return [word.strip() for word in jieba.cut(sentence.strip())\n",
    "            if word and word not in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['短片', '过激', '目前为止', '一声', '谢谢']\n"
     ]
    }
   ],
   "source": [
    "print(wordCut('这个短片过激了啊，以目前为止你帮助别人会获得一声谢谢的', stop_words='data/stopwords.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createVocabList(dmList):\n",
    "    ##创建一个空集\n",
    "    myVocabList = set([])\n",
    "    #将新词集合添加到创建的集合中\n",
    "    for dm in dmList:\n",
    "        #操作符 | 用于求两个集合的并集\n",
    "        myVocabList = myVocabList | set(dm)\n",
    "    #返回一个包含所有文档中出现的不重复词的列表\n",
    "    return list(myVocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['悟出', '受伤害', '做事', '感觉', '机器人', '广州', '慢慢', '爱国', '骗', '公交', '爷爷奶奶', '妇女', '让座', '快到', '一对', '所有人', '一声', '道', '不错', '单细胞', '偶然', '做好事', '这周', '敬业', '反思', '丧失', '而立之年', '以偏概全', '奥', '理想', '碍', '太', '做', '恶毒', '惨', '一部分', '自我', '远古', '纳尼', '主动', '人心', '目前为止', '生物', '下车', '学', '说', '爷爷', '无情', '冷漠', '机器', '善良', '运气', '确实', '学会', '天狗', '道谢', '小朋友', '没救', '谢谢', '真的', '冷嘲热讽', '友善', '诚信', '短片', '站', '温度', '过激', '抢', '想', '世间', '这小', '好不容易', '本来', '社会']\n"
     ]
    }
   ],
   "source": [
    "pos_sentence_list, neg_sentence_list = load_data('data/pos_file.txt','data/neg_file.txt')\n",
    "sentences = []\n",
    "# 把正负样本集合到一起\n",
    "for sentence in pos_sentence_list:\n",
    "    sentence = wordCut(sentence)\n",
    "    sentences.append(sentence)\n",
    "for sentence in neg_sentence_list:\n",
    "    sentence = wordCut(sentence)\n",
    "    sentences.append(sentence)\n",
    "label = [1] * len(pos_sentence_list) + [0] * len(neg_sentence_list)  # 每个正负样本对应的类别\n",
    "vocabList = createVocabList(sentences)\n",
    "print(vocabList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setOfWords2Vec(vocabList, inputSet):\n",
    "    #把一组词转换成词向量\n",
    "    #返回值：文档向量\n",
    "    #首先创建一个元素都为0的向量，长度等于词汇表的长度\n",
    "    returnVec = [0]*len(vocabList)\n",
    "    #遍历文档中词汇\n",
    "    for word in inputSet:\n",
    "        #如果文档中的单词在词汇表中，则相应向量位置置1\n",
    "        if word in vocabList:            \n",
    "            returnVec[vocabList.index(word)] = 1\n",
    "        #否则输出打印信息\n",
    "        else: print(\"the word: %s is not in my Vocablary!\" % word)\n",
    "    #向量的每一个元素为1或0，表示词汇表中的单词在文档中是否出现\n",
    "    return returnVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['短片', '过激', '目前为止', '一声', '谢谢']\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "dmVoca=wordCut('这个短片过激了啊，以目前为止你帮助别人会获得一声谢谢的', \n",
    "               stop_words='data\\stopwords.txt')\n",
    "Vec=setOfWords2Vec(vocabList, dmVoca)\n",
    "print(dmVoca)\n",
    "print(Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train( trainMatrix, label):\n",
    "    #获得训练集中弹幕个数\n",
    "    numTrainDanmu  = len(trainMatrix)\n",
    "    # 分别统计正面和负面弹幕中每个词出现的次数\n",
    "    # 默认每个词出现至少一次\n",
    "    pos_each_word = np.ones(len(trainMatrix[0])) \n",
    "    neg_each_word = np.ones(len(trainMatrix[0])) \n",
    "    pos_words = 2.0 # 正面弹幕中所有词的总数\n",
    "    neg_words = 2.0 # 负面弹幕中所有词的总数\n",
    "    #遍历训练集trainMatrix中所有弹幕\n",
    "    for i in range(numTrainDanmu):\n",
    "         #如果正面样本，则累加到正向词向量，且修改正向弹幕的总词数\n",
    "        if label[i] == 1:            \n",
    "            pos_each_word += trainMatrix[i]\n",
    "            pos_words += sum(trainMatrix[i])\n",
    "        #如果负面样本，则累加到负向词向量，且修改负向弹幕的总词数    \n",
    "        if label[i] == 0:\n",
    "            neg_each_word += trainMatrix[i]\n",
    "            neg_words += sum(trainMatrix[i])\n",
    "    # 对每个词出现的概率取对数\n",
    "    #这是因为大多数概率都很小，如果再对很多很小的数进行乘法操作，会下溢或得不得正确答案。\n",
    "    # 比如a=0.001， b=0.0003, a*b = exp(ln(a)+ln(b))\n",
    "    pos_each_word_prob = np.log(pos_each_word/pos_words)\n",
    "    neg_each_word_prob = np.log(neg_each_word/neg_words)\n",
    "    #对每个元素做除法求概率\n",
    "       #返回两个类别概率向量和一个概率\n",
    "    return pos_each_word_prob, neg_each_word_prob, \\\n",
    "        sum(label)/numTrainDanmu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(test_data, pos_each_word_prob, neg_each_word_prob, pos_prob):\n",
    "    #测试数据，分别求出两个类别的概率\n",
    "    #向量元素相乘后求和再加到类别的对数概率上，等价于概率相乘\n",
    "    p1 = sum(test_data*pos_each_word_prob) + np.log(pos_prob)\n",
    "    p0 = sum(test_data*neg_each_word_prob) + np.log(1-pos_prob)\n",
    "    #将新弹幕判定为概率大的类别\n",
    "    if p1 > p0:\n",
    "        return 1, np.exp(p1), np.exp(p0)\n",
    "    else:\n",
    "        return 0, np.exp(p1), np.exp(p0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将来的社会，人类会更加冷漠\r\n",
      "\n",
      "the word: 将来 is not in my Vocablary!\n",
      "the word: 人类 is not in my Vocablary!\n",
      "(0, 0.0003858024691358025, 0.00257201646090535)\n",
      "别人帮助你，应该说是“谢谢”\n",
      "(1, 0.0077160493827160455, 0.00017146776406035659)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pos_danmu_list, neg_danmu_list = load_data('data/pos_file.txt',\\\n",
    "                                               'data/neg_file.txt')\n",
    "    danmuList = []\n",
    "    # 把正负样本集合到一起\n",
    "    for danmu in pos_danmu_list:\n",
    "        danmu = wordCut(danmu)\n",
    "        danmuList.append(danmu)\n",
    "    for danmu in neg_danmu_list:\n",
    "        danmu = wordCut(danmu)\n",
    "        danmuList.append(danmu)\n",
    "    # 得到所有样本label列表\n",
    "    label = [1] * len(pos_danmu_list) + [0] * len(neg_danmu_list)\n",
    "    #计算词汇表\n",
    "    myvocabList = createVocabList(danmuList)\n",
    "    #将弹幕数据集分词，并转化为向量\n",
    "    train_data = []\n",
    "    for danmu in danmuList:\n",
    "        trainMat = setOfWords2Vec(myvocabList, danmu)\n",
    "        train_data.append(trainMat)\n",
    "    # 利用训练集，求出每个类别中各单词的条件概率\n",
    "    pos_each_word_prob, neg_each_word_prob, pos_prob = train(train_data, label)\n",
    "    #预测新弹幕\n",
    "    test_sentences = codecs.open(\"data/test_file.txt\", 'r', 'utf-8').readlines()\n",
    "    for sentence in test_sentences:\n",
    "        print(sentence)  #输出弹幕\n",
    "        sentence = wordCut(sentence)                      #分词\n",
    "        test_vec = setOfWords2Vec(myvocabList, sentence)  #转换为向量\n",
    "        pred = classify(np.array(test_vec), pos_each_word_prob, \\\n",
    "                        neg_each_word_prob, pos_prob)\n",
    "        print(pred)  #输出预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "将来的社会，人类会更加冷漠\r\n",
      "\n",
      "the word: 将来 is not in my Vocablary!\n",
      "the word: 人类 is not in my Vocablary!\n",
      "[0]\n",
      "别人帮助你，应该说是“谢谢”\n",
      "[1]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "clf = GaussianNB()\n",
    "#拟合数据\n",
    "clf.fit(train_data, label)\n",
    "for sentence in test_sentences:\n",
    "    print(sentence)  #输出弹幕\n",
    "    sentence = wordCut(sentence)                      #分词\n",
    "    test_vec = setOfWords2Vec(myvocabList, sentence)  #转换为向量\n",
    "    pred = clf.predict([test_vec])\n",
    "    print(pred)  #输出预测值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
